LSTM performance, 88% at peak.

Convnet
Able to get to as low as .16 error with 1 conv. starts to diverage @ epoch 21?
Does not seem to overfit at all. attempted to fix this by adding another conv layer.
  As expected, adding this second layer rapidly speeds up training. .20 test @ epoch 3.
  Still no overfitting at 7 epoch. @ .1522 misclass
  These two conv layers now exceed older, ~.135 error on test. Fairly noisy though. 
  Next epoch .150 error.

  Second conv layer model gets like 88%, but this was only on 50d word embedding. Bumping up to 300 to see what happens.
300d, able to get .10 error
Data usage is huge. Lets try to use fuels new data server thinngy.
  Now super quick.

With the maxpools, started to overfit quicker.
Added dropout on last fullyconnected
Didn't do all that much. Still overfitting, same way.

Lets speed this up. Looks like conv2d and max_pool arn't using dnn.
  Quick speed test shows dnn faster.

Tried the weighted sigmoid, thought would train faster.
Didn't actually do this. Makes me think that the network is not at all learning what I want it to.
Need to trick it with good initializations.

Bricks API is rather restrictive, I think I am going to make a wrapper
that takes into account custom initialization as well as the fork needed to
get the units in the right shape.
Will be a GRULayer or something, not just the recurrent pieces.
Also split off two different weights in there.

So the really simple linear net works really well...
input -> linear -> sum sequence -> linear to score
like 88%
